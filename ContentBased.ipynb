{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "        Load events from files and convert to dataframe.\n",
    "    \"\"\"\n",
    "    map_lst=[]\n",
    "    for f in os.listdir(path):\n",
    "        file_name=os.path.join(path,f)\n",
    "        if os.path.isfile(file_name):\n",
    "            for line in open(file_name):\n",
    "                obj = json.loads(line.strip())\n",
    "                if not obj is None:\n",
    "                    map_lst.append(obj)\n",
    "    return pd.DataFrame(map_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used to process input data\n",
    "def make_lower_case(text):\n",
    "    \"\"\"\n",
    "        Process text into lower case.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return text.lower()\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "def remove_stop_words(text):\n",
    "    \"\"\"\n",
    "        Remove stop words from text.\n",
    "    \"\"\"\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"norwegian\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    texts = [w for w in text if w.isalpha()]\n",
    "    texts = \" \".join(texts)\n",
    "    return texts\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "        Remove punctuation from text\n",
    "    \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    \"\"\"\n",
    "        Remove HTML tags from text\n",
    "    \"\"\"\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_processing(df):\n",
    "    \"\"\"\n",
    "        Applying text processing to the text contained in each title\n",
    "    \"\"\"\n",
    "    df = df.loc[(df[\"url\"] != \"http://adressa.no\")]\n",
    "    df = df.dropna(subset=['title'])\n",
    "    df = df.drop_duplicates(subset='title', keep=\"first\")\n",
    "\n",
    "    df['cleaned_title'] = df['title'].apply(func = make_lower_case)\n",
    "    df['cleaned_title'] = df.cleaned_title.apply(func = remove_stop_words)\n",
    "    df['cleaned_title'] = df.cleaned_title.apply(func = remove_punctuation)\n",
    "    df['cleaned_title'] = df.cleaned_title.apply(func = remove_html)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data into df\n",
    "df = load_data(\"active1000\")\n",
    "\n",
    "#Processing of data to remove stop words, punctuation, and other irrelevant information\n",
    "df_updated = content_processing(df)\n",
    "\n",
    "#Split data into a traing and testing set \n",
    "train_data, test_data = train_test_split(df_updated, test_size=0.2, random_state=42)\n",
    "\n",
    "#Vectorize the training data using the processed title\n",
    "tfidf = TfidfVectorizer()\n",
    "train_tfidf_matrix = tfidf.fit_transform(train_data['cleaned_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles(article, index, number):\n",
    "    \"\"\"\n",
    "        Returns a number of recommended articles to the article provided\n",
    "    \"\"\"\n",
    "    # vectorize the article keywords\n",
    "    article_vec = tfidf.transform([article])\n",
    "    # calculate the similarity\n",
    "    sim_scores = cosine_similarity(article_vec, train_tfidf_matrix)\n",
    "    # get the most similar articles\n",
    "    sim_scores = sim_scores[index]\n",
    "    article_indices = sim_scores.argsort()[::-1][:number]\n",
    "    article_indices = np.array(article_indices).reshape(-1)\n",
    "    return train_data['title'].iloc[article_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2207202    Her fortviler Jarstein etter scoringen som kan...\n",
      "2099171    «Vi skal til Champions League med Molde før vi...\n",
      "1936005                           Her er laget ditt, Nilsen!\n",
      "1345646    Ble oppdaget av landslagsstjerne – nå er sørle...\n",
      "76743                               Dette kan koste deg dyrt\n",
      "1854612               Se Champions League-trekningen direkte\n",
      "1788643    Leicester-eventyret fortsetter: Til kvartfinal...\n",
      "195540                      Vil ha kun de beste med på laget\n",
      "1152243                      Mathallens stamkunder fortviler\n",
      "1154       Her er scoringen som holder Liverpool inne i g...\n",
      "2031225      Jarstein var uaktuell som kaptein for Lagerbäck\n",
      "477585            Nå er Elabdellaoui klar for Premier League\n",
      "426886     Kjøreturen mellom Trondheim og Steinkjer kan k...\n",
      "1964063    Denne superbussveien på 750 meter kan koste 50...\n",
      "1142043    Toppdommeren bytter ut Premier League med Saud...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Example user\n",
    "user_id = \"cx:ib1vo01vq38f2mqc:20lut6o1pv35i\"  \n",
    "user_data = df[df['userId'] == user_id]\n",
    "last_article = user_data['title'].iloc[-1]\n",
    "\n",
    "#Recommend articles to a user based on their last read artivle\n",
    "print(recommend_articles(last_article, 0, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(df): \n",
    "    \"\"\"\n",
    "        Evaluation function that prints average precision, recall and f1 score to a given data set\n",
    "    \"\"\"\n",
    "    # Split the data into a training set and a test set\n",
    "    holdout = test_data\n",
    "    train = train_data\n",
    "\n",
    "    # Evaluate the recommender system on the holdout set\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    for user, article_title in holdout[['userId', 'title']].values:\n",
    "\n",
    "        # Get the ground-truth similar articles for the current user and article\n",
    "        ground_truth = df[(df['userId'] == user) & (df['title'] != article_title)]['title'].unique()\n",
    "        \n",
    "        # Get the index of the current article in the training set\n",
    "        if article_title in train['title'].values:\n",
    "            idx = train[train['title'] == article_title].index[0]\n",
    "        else:\n",
    "            idx = None\n",
    "            \n",
    "        top_titles = recommend_articles(article_title, idx)\n",
    "        \n",
    "        # Compute the precision and recall for the recommended articles\n",
    "        relevant = set(ground_truth)\n",
    "        retrieved = set(top_titles)\n",
    "        intersection = relevant.intersection(retrieved)\n",
    "        precision = len(intersection) / len(retrieved)\n",
    "        if len(relevant) > 0:\n",
    "            recall = len(intersection) / len(relevant)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "    # Compute the average precision, recall, and F1 score\n",
    "    avg_precision = sum(precision_list) / len(precision_list)\n",
    "    avg_recall = sum(recall_list) / len(recall_list)\n",
    "    f1_score = 2 * ((avg_precision * avg_recall) / (avg_precision + avg_recall))\n",
    "\n",
    "    print(\"Average precision: \" +str(avg_precision))\n",
    "    print(\"Average recall: \" +str(avg_recall))\n",
    "    print(\"Average f1 score: \" +str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision: 0.10387735780290482\n",
      "Average recall: 0.7991647944733982\n",
      "Average f1 score: 0.1838565942680337\n"
     ]
    }
   ],
   "source": [
    "eval(df=df_updated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4215",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42ab7749d3f817ca94801beb84f53dddca94ec172cf65b3c65ab5d73a1b15442"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
